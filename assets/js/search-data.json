{
  
    
        "post0": {
            "title": "ASHRAE Energy Prediction",
            "content": ". The ASHRAE Great Energy Predictor III is a Kaggle competition to predict energy use in buildings. . The training dataset is 3.2GB, containing 20 million energy meter readings for 1449 buildings over a year. Four energy meters are sampled: chilled water, electric, hot water, and steam. Each dependent measurement is given with independent variable metadata for the building and weather. The Kaggle data page shows the available files, with histograms for each variable. . Our goal was to create a model that predicts energy usage per building and per meter in that building, using fast.ai. . We started by downloading the dataset from Kaggle, joining the tables, fixing timestamp inconsistencies and transforming some values into smaller datatypes to save memory. We aimed to work in Colab so memory was precious. We serialised this data preparation to a Parquet file on our Google Drives. . !pip install -Uqq fastbook dtreeviz import fastbook fastbook.setup_book() . . |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 720 kB 13.4 MB/s |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60 kB 5.9 MB/s |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 188 kB 20.7 MB/s |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 46 kB 3.3 MB/s |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.2 MB 27.0 MB/s |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 53 kB 1.8 MB/s |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 51 kB 259 kB/s Building wheel for dtreeviz (setup.py) ... done Mounted at /content/gdrive . import os import gc import pandas as pd import datetime as dt import seaborn as sns from tqdm.auto import tqdm from pandas.api.types import is_string_dtype, is_numeric_dtype, is_categorical_dtype from fastai.tabular.all import * from sklearn.ensemble import RandomForestRegressor from sklearn.tree import DecisionTreeRegressor from dtreeviz.trees import * from IPython.display import Image, display_svg, SVG from sklearn.metrics import mean_squared_log_error #mpl.rcParams[&#39;figure.dpi&#39;] = 100 sns.set() pd.options.display.max_rows = 20 pd.options.display.max_columns = 8 . . %cd &#39;/content/gdrive/MyDrive/Colab Notebooks/ashrae&#39; train_valid = pd.read_parquet(&quot;feature_enhanced_train_combined.parquet.snappy&quot;) . . /content/gdrive/MyDrive/Colab Notebooks/ashrae . ## Memory optimization # Original code from https://www.kaggle.com/gemartin/load-data-reduce-memory-usage by @gemartin # Modified to support timestamp type, categorical type # Modified to add option to use float16 from pandas.api.types import is_datetime64_any_dtype as is_datetime from pandas.api.types import is_categorical_dtype def reduce_mem_usage(df, use_float16=False): &quot;&quot;&quot; Iterate through all the columns of a dataframe and modify the data type to reduce memory usage. &quot;&quot;&quot; start_mem = df.memory_usage().sum() / 1024**2 print(&quot;Memory usage of dataframe is {:.2f} MB&quot;.format(start_mem)) for col in df.columns: if is_datetime(df[col]) or is_categorical_dtype(df[col]) or is_string_dtype(df[col]): continue col_type = df[col].dtype if col_type != object: c_min = df[col].min() c_max = df[col].max() if str(col_type)[:3] == &quot;int&quot;: if c_min &gt; np.iinfo(np.int8).min and c_max &lt; np.iinfo(np.int8).max: df[col] = df[col].astype(np.int8) elif c_min &gt; np.iinfo(np.int16).min and c_max &lt; np.iinfo(np.int16).max: df[col] = df[col].astype(np.int16) elif c_min &gt; np.iinfo(np.int32).min and c_max &lt; np.iinfo(np.int32).max: df[col] = df[col].astype(np.int32) elif c_min &gt; np.iinfo(np.int64).min and c_max &lt; np.iinfo(np.int64).max: df[col] = df[col].astype(np.int64) else: if use_float16 and c_min &gt; np.finfo(np.float16).min and c_max &lt; np.finfo(np.float16).max: df[col] = df[col].astype(np.float16) elif c_min &gt; np.finfo(np.float32).min and c_max &lt; np.finfo(np.float32).max: df[col] = df[col].astype(np.float32) else: df[col] = df[col].astype(np.float64) else: df[col] = df[col].astype(&quot;category&quot;) end_mem = df.memory_usage().sum() / 1024**2 print(&quot;Memory usage after optimization is: {:.2f} MB&quot;.format(end_mem)) print(&quot;Decreased by {:.1f}%&quot;.format(100 * (start_mem - end_mem) / start_mem)) return df train_valid = reduce_mem_usage(train_valid, use_float16=True) . . NameError Traceback (most recent call last) &lt;ipython-input-3-c1807509a82e&gt; in &lt;module&gt;() 50 return df 51 &gt; 52 train_valid = reduce_mem_usage(train_valid, use_float16=True) NameError: name &#39;train_valid&#39; is not defined . Exploratory Data Analysis . We started by browsing exploratory data analysis notebooks posted to Kaggle. We learnt that this is a gnarly dataset, with many missing values and outliers. For example, the &#39;meter_reading&#39; dependent variable has mostly small values but also some very large values: . train_valid[[&#39;meter_reading&#39;]].hist() . array([[&lt;matplotlib.axes._subplots.AxesSubplot object at 0x7fc260160550&gt;]], dtype=object) . Exploring further (with credit to https://www.kaggle.com/nroman/eda-for-ashrae) we discover that most of the energy in the dataset is consumed by a single building and single meter, building_id 1099 meter 2. The following plots show daily averaged meter readings for: . the full dataset | just building_id 1099 meter 2 | the dataset with building_id 1099 removed | . It is tempting to model this building independently from the rest of the dataset, to avoid numerical issues and overfitting to this building. . fig, axes = plt.subplots(3,1,figsize=(14, 20), dpi=100) #train_valid[[&#39;timestamp&#39;, &#39;meter_reading&#39;]].set_index(&#39;timestamp&#39;).resample(&#39;H&#39;).mean()[&#39;meter_reading&#39;].plot(ax=axes[1], alpha=0.8, label=&#39;By hour&#39;, color=&#39;tab:blue&#39;).set_ylabel(&#39;Mean meter reading&#39;, fontsize=13); train_valid[[&#39;timestamp&#39;, &#39;meter_reading&#39;]].set_index(&#39;timestamp&#39;).resample(&#39;D&#39;).mean()[&#39;meter_reading&#39;].plot(ax=axes[1], alpha=1, label=&#39;By day&#39;, color=&#39;tab:orange&#39;).set_ylabel(&#39;Mean meter reading&#39;, fontsize=13); #train_valid[(train_valid[&#39;meter&#39;] == 2) &amp; (train_valid[&#39;building_id&#39;] == 1099)][[&#39;timestamp&#39;, &#39;meter_reading&#39;]].set_index(&#39;timestamp&#39;).resample(&#39;H&#39;).mean()[&#39;meter_reading&#39;].plot(ax=axes[0], alpha=0.8, label=&#39;By hour&#39;, color=&#39;tab:blue&#39;).set_ylabel(&#39;Mean meter reading&#39;, fontsize=13); train_valid[(train_valid[&#39;meter&#39;] == 2) &amp; (train_valid[&#39;building_id&#39;] == 1099)][[&#39;timestamp&#39;, &#39;meter_reading&#39;]].set_index(&#39;timestamp&#39;).resample(&#39;D&#39;).mean()[&#39;meter_reading&#39;].plot(ax=axes[0], alpha=1, label=&#39;By day&#39;, color=&#39;tab:orange&#39;).set_ylabel(&#39;Mean meter reading&#39;, fontsize=13); #train_valid[~((train_valid[&#39;meter&#39;] == 2) &amp; (train_valid[&#39;building_id&#39;] == 1099))][[&#39;timestamp&#39;, &#39;meter_reading&#39;]].set_index(&#39;timestamp&#39;).resample(&#39;H&#39;).mean()[&#39;meter_reading&#39;].plot(ax=axes[2], alpha=0.8, label=&#39;By hour&#39;, color=&#39;tab:blue&#39;).set_ylabel(&#39;Mean meter reading&#39;, fontsize=13); train_valid[~((train_valid[&#39;meter&#39;] == 2) &amp; (train_valid[&#39;building_id&#39;] == 1099))][[&#39;timestamp&#39;, &#39;meter_reading&#39;]].set_index(&#39;timestamp&#39;).resample(&#39;D&#39;).mean()[&#39;meter_reading&#39;].plot(ax=axes[2], alpha=1, label=&#39;By day&#39;, color=&#39;tab:orange&#39;).set_ylabel(&#39;Mean meter reading&#39;, fontsize=13); axes[0].set_title(&#39;Full dataset&#39;, fontsize=13); axes[1].set_title(&#39;building_id==1099 and meter==2&#39;, fontsize=13); axes[2].set_title(&#39;building_id 1099 excluded&#39;, fontsize=13); plt.subplots_adjust(hspace=0.45) . . Splitting into Training and Validation sets . Given that we have a year&#39;s worth of time series data, we investigated several methods for splitting into training and validation sets. We ruled out random splits but considered periodic splits such as taking every 4th week for validation. In the end, we opted for a simple 2-fold time-based cross-validation, taking the first 6 months of data to train one model and the second 6 months for a second model, averaging predictions from each. . Data Transforms . We started with the default trio of fast.ai transforms: Categorify, FillMissing, Normalize. We experimented with omitting each and saw our model performance degrade. We also experimented with dropping rows that were missing the key independent variables, since there aren&#39;t many such rows. This approach gave good performance so we would consider this when tuning our model. . Feature Engineering . We engineered some additional time features from the &#39;timestamp&#39; variable, namely: . year_fraction | week_fraction | day_fraction We found through feature clustering that &#39;year_fraction&#39; was highly correlated with &#39;air_temperature&#39; which makes sense because air temperature is a good proxy for month of the year. We omitted &#39;year_fraction&#39; to simplify the model. | . # Get a subset of the data, a few buildings and all meters #train = train_valid[(train_valid[&#39;building_id&#39;]&gt;=1126)] train = train_valid del train_valid gc.collect() . . 103 . The code below shows how we created the DataLoaders, using only a subset of the most important features: . # 2-fold cross-validation. Build 2 models, each with half the year as training data splits1=MaskSplitter(train[&#39;timestamp&#39;] &gt;= &#39;2016-07-01&#39;)(range_of(train)) cat_names = [&#39;building_id&#39;, &#39;meter&#39;, &#39;site_id&#39;, &#39;primary_use&#39;] cont_names = [&#39;air_temperature&#39;, &#39;dew_temperature&#39;, &#39;square_feet&#39;, &#39;week_fraction&#39;, &#39;day_fraction&#39;] procs = [Categorify, FillMissing, Normalize] to1 = TabularPandas(train, procs=procs, cat_names = cat_names, cont_names = cont_names, y_names=&#39;meter_reading&#39;, splits=splits1) dls1 = to1.dataloaders(bs=1024) gc.collect() . 46 . Feature Importance . We trained a scikit-learn DecisionTreeRegressor to compare against our neural network and to assess feature importance. . We discovered that the most significant features influencing energy usage for a particular building are: . &#39;primary_use&#39; surprisingly, whether a building is used for education, office, retail etc has a large influence on energy use | &#39;square_feet&#39; representing the size of the building | &#39;air_temperature&#39; which is unsurprising | &#39;dew_temperature&#39; is not as correlated with air temperature as you would expect | &#39;day_fraction&#39; meaning that energy is consumed at regular times every day | . # Train a fast.ai decision tree xs,y = to1.train.xs,to1.train.y valid_xs,valid_y = to1.valid.xs,to1.valid.y m = DecisionTreeRegressor(min_samples_leaf=1000) m.fit(xs, y); . . def rf_feat_importance(m, df): return pd.DataFrame({&#39;feature&#39;:df.columns, &#39;importance&#39;:m.feature_importances_} ).sort_values(&#39;importance&#39;, ascending=False) fi = rf_feat_importance(m, xs) fi[:10] . . xs,y = to1.train.xs,to1.train.y valid_xs,valid_y = to1.valid.xs,to1.valid.y print(&quot;Training set RMSLE: &quot; + str(mean_squared_log_error(y, m.predict(xs)))) print(&quot;Validation set RMSLE: &quot; + str(mean_squared_log_error(valid_y, m.predict(valid_xs)))) building=2 meter=2 #plt.scatter(valid_xs[(valid_xs[&#39;building_id&#39;]==building) &amp; (valid_xs[&#39;meter&#39;]==meter)][&#39;air_temperature&#39;], valid_y.loc[valid_xs[(valid_xs[&#39;building_id&#39;]==building) &amp; (valid_xs[&#39;meter&#39;]==meter)].index]) #plt.scatter(valid_xs[(valid_xs[&#39;building_id&#39;]==building) &amp; (valid_xs[&#39;meter&#39;]==meter)][&#39;air_temperature&#39;], pd.DataFrame(valid_y_pred, index=valid_y.index).loc[valid_xs[(valid_xs[&#39;building_id&#39;]==building) &amp; (valid_xs[&#39;meter&#39;]==meter)].index], color=&#39;orange&#39;) . . Learning Rates . We tried using lr_find() to choose a learning rate. However, there is so much hetrogeneity in the data that lr_find() doesn&#39;t give the lovely smooth curves in the documentation. Moreover, we found that the suggested learning rates were too high, resulting in divergence when training. . # learn1 = (&#39;learn1.pkl&#39;).load() learn1 = tabular_learner(dls1, metrics=rmse) learn1.summary() # Exploring learning rates learn1.lr_find(stop_div=False, num_it=200) . SuggestedLRs(valley=0.3981071710586548) . Model Training . So, we trained a bunch of models, experimenting with all the hyperparameters discussed, including: . Features | Batch sizes | Learning rates | Weight decay | Number of buildings included | . learn1.fit_one_cycle(5, lr_max=.00001) save_pickle(&#39;learn1.pkl&#39;,learn1) . . 20.00% [1/5 06:18&lt;25:12] epoch train_loss valid_loss _rmse time . 0 | 43956633600.000000 | 1217302784.000000 | 34890.003906 | 06:18 | . . 0.00% [0/9612 00:00&lt;00:00] &lt;/div&gt; &lt;/div&gt; KeyboardInterrupt Traceback (most recent call last) &lt;ipython-input-8-df518c5aab77&gt; in &lt;module&gt;() -&gt; 1 learn1.fit_one_cycle(5, lr_max=.00001) 2 save_pickle(&#39;learn1.pkl&#39;,learn) /usr/local/lib/python3.7/dist-packages/fastai/callback/schedule.py in fit_one_cycle(self, n_epoch, lr_max, div, div_final, pct_start, wd, moms, cbs, reset_opt) 111 scheds = {&#39;lr&#39;: combined_cos(pct_start, lr_max/div, lr_max, lr_max/div_final), 112 &#39;mom&#39;: combined_cos(pct_start, *(self.moms if moms is None else moms))} --&gt; 113 self.fit(n_epoch, cbs=ParamScheduler(scheds)+L(cbs), reset_opt=reset_opt, wd=wd) 114 115 # Cell /usr/local/lib/python3.7/dist-packages/fastai/learner.py in fit(self, n_epoch, lr, wd, cbs, reset_opt) 219 self.opt.set_hypers(lr=self.lr if lr is None else lr) 220 self.n_epoch = n_epoch --&gt; 221 self._with_events(self._do_fit, &#39;fit&#39;, CancelFitException, self._end_cleanup) 222 223 def _end_cleanup(self): self.dl,self.xb,self.yb,self.pred,self.loss = None,(None,),(None,),None,None /usr/local/lib/python3.7/dist-packages/fastai/learner.py in _with_events(self, f, event_type, ex, final) 161 162 def _with_events(self, f, event_type, ex, final=noop): --&gt; 163 try: self(f&#39;before_{event_type}&#39;); f() 164 except ex: self(f&#39;after_cancel_{event_type}&#39;) 165 self(f&#39;after_{event_type}&#39;); final() /usr/local/lib/python3.7/dist-packages/fastai/learner.py in _do_fit(self) 210 for epoch in range(self.n_epoch): 211 self.epoch=epoch --&gt; 212 self._with_events(self._do_epoch, &#39;epoch&#39;, CancelEpochException) 213 214 def fit(self, n_epoch, lr=None, wd=None, cbs=None, reset_opt=False): /usr/local/lib/python3.7/dist-packages/fastai/learner.py in _with_events(self, f, event_type, ex, final) 161 162 def _with_events(self, f, event_type, ex, final=noop): --&gt; 163 try: self(f&#39;before_{event_type}&#39;); f() 164 except ex: self(f&#39;after_cancel_{event_type}&#39;) 165 self(f&#39;after_{event_type}&#39;); final() /usr/local/lib/python3.7/dist-packages/fastai/learner.py in _do_epoch(self) 204 205 def _do_epoch(self): --&gt; 206 self._do_epoch_train() 207 self._do_epoch_validate() 208 /usr/local/lib/python3.7/dist-packages/fastai/learner.py in _do_epoch_train(self) 196 def _do_epoch_train(self): 197 self.dl = self.dls.train --&gt; 198 self._with_events(self.all_batches, &#39;train&#39;, CancelTrainException) 199 200 def _do_epoch_validate(self, ds_idx=1, dl=None): /usr/local/lib/python3.7/dist-packages/fastai/learner.py in _with_events(self, f, event_type, ex, final) 161 162 def _with_events(self, f, event_type, ex, final=noop): --&gt; 163 try: self(f&#39;before_{event_type}&#39;); f() 164 except ex: self(f&#39;after_cancel_{event_type}&#39;) 165 self(f&#39;after_{event_type}&#39;); final() /usr/local/lib/python3.7/dist-packages/fastai/learner.py in all_batches(self) 167 def all_batches(self): 168 self.n_iter = len(self.dl) --&gt; 169 for o in enumerate(self.dl): self.one_batch(*o) 170 171 def _do_one_batch(self): /usr/local/lib/python3.7/dist-packages/fastai/data/load.py in __iter__(self) 106 self.randomize() 107 self.before_iter() --&gt; 108 self.__idxs=self.get_idxs() # called in context of main process (not workers/subprocesses) 109 for b in _loaders[self.fake_l.num_workers==0](self.fake_l): 110 if self.device is not None: b = to_device(b, self.device) /usr/local/lib/python3.7/dist-packages/fastai/data/load.py in get_idxs(self) 97 idxs = Inf.count if self.indexed else Inf.nones 98 if self.n is not None: idxs = list(itertools.islice(idxs, self.n)) &gt; 99 if self.shuffle: idxs = self.shuffle_fn(idxs) 100 return idxs 101 /usr/local/lib/python3.7/dist-packages/fastai/data/load.py in shuffle_fn(self, idxs) 134 except SkipItemException: return None 135 def chunkify(self, b): return b if self.prebatched else chunked(b, self.bs, self.drop_last) --&gt; 136 def shuffle_fn(self, idxs): return self.rng.sample(idxs, len(idxs)) 137 def randomize(self): self.rng = random.Random(self.rng.randint(0,2**32-1)) 138 def retain(self, res, b): return retain_types(res, b[0] if is_listy(b) else b) /usr/lib/python3.7/random.py in sample(self, population, k) 330 j = randbelow(n-i) 331 result[i] = pool[j] --&gt; 332 pool[j] = pool[n-i-1] # move non-selected item into vacancy 333 else: 334 selected = set() KeyboardInterrupt: . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; save_pickle(&#39;learn1.pkl&#39;,learn1) . Metrics . The competition stated that that chosen metric was root mean squared log error: RMSLE=sqrt(1/ùëõ‚àë(log(ùëù+1)‚àílog(ùëé+1))**2) . fast.ai doesn&#39;t have this metric but does have mean squared log error, MSLE. The catch is that RMSLE and MSLE do not handle negative numbers, returning -‚àû. Our neural network predicts some negative numbers for meter readings that should be 0. So we chose to train our model using root mean squared error (RMSE) as our metric and then use RMSLE with negative values set to 0 when evaluating our performance manually post-training. . learn.show_results() from sklearn.metrics import mean_squared_log_error xs,y = to1.train.xs,to1.train.y valid_xs,valid_y = to1.valid.xs,to1.valid.y y_pred,y_targs = learn.get_preds(ds_idx=0) valid_y_pred,valid_y_targs = learn.get_preds() # Set negative values to 0 because the RMSLE metric can&#39;t handle negatives y_pred[y_pred&lt;0]=0 valid_y_pred[valid_y_pred&lt;0]=0 print(&quot;Training set RMSLE: &quot; + str(np.sqrt(mean_squared_log_error(y_pred.detach().numpy(), y)))) print(&quot;Validation set RMSLE: &quot; + str(np.sqrt(mean_squared_log_error(valid_y_pred.detach().numpy(), valid_y)))) building=2 meter=2 plt.scatter(valid_xs[(valid_xs[&#39;building_id&#39;]==building) &amp; (valid_xs[&#39;meter&#39;]==meter)][&#39;air_temperature&#39;], valid_y.loc[valid_xs[(valid_xs[&#39;building_id&#39;]==building) &amp; (valid_xs[&#39;meter&#39;]==meter)].index]) plt.scatter(valid_xs[(valid_xs[&#39;building_id&#39;]==building) &amp; (valid_xs[&#39;meter&#39;]==meter)][&#39;air_temperature&#39;], pd.DataFrame(valid_y_pred, index=valid_y.index).loc[valid_xs[(valid_xs[&#39;building_id&#39;]==building) &amp; (valid_xs[&#39;meter&#39;]==meter)].index], color=&#39;orange&#39;) . We achieved a validation set RMSLE of 4.51, compared to validation set RMSLE of 2.02 for our DecisionTreeRegressor. . The winning Kaggle entry scored RMSLE of 1.23. Assuming that our cross-validated sets are representative of the test set (a big assumption!) this puts us around 3000th on the Kaggle leaderboard of 3500 entries. That&#39;s not great but not embarrasing either. . if &#39;to1&#39; in locals(): del to1 if &#39;dls1&#39; in locals(): del dls1 %cd &#39;/content/gdrive/MyDrive/Colab Notebooks/ashrae&#39; train = pd.read_parquet(&quot;feature_enhanced_train_combined.parquet.snappy&quot;) train = reduce_mem_usage(train, use_float16=True) gc.collect() # Use an splitter and load using TabularPandas then to a dataloaders # 2-fold cross-validation. Build 2 models, each with half the year as training data splits2=MaskSplitter(train[&#39;timestamp&#39;] &lt; &#39;2016-07-01&#39;)(range_of(train)) cat_names = [&#39;building_id&#39;, &#39;meter&#39;, &#39;site_id&#39;, &#39;primary_use&#39;] cont_names = [&#39;air_temperature&#39;, &#39;dew_temperature&#39;, &#39;square_feet&#39;, &#39;week_fraction&#39;, &#39;day_fraction&#39;] procs = [Categorify, FillMissing, Normalize] to2 = TabularPandas(train, procs=procs, cat_names = cat_names, cont_names = cont_names, y_names=&#39;meter_reading&#39;, splits=splits2) dls2 = to2.dataloaders(bs=2048) del train gc.collect() . . /content/gdrive/MyDrive/Colab Notebooks/ashrae Memory usage of dataframe is 3243.66 MB Memory usage after optimization is: 1189.98 MB Decreased by 63.3% . 70 . # Train a tabular model # learn2 = (&#39;learn2.pkl&#39;).load() learn2 = tabular_learner(dls2, metrics=rmse) learn2.summary() #learn2.lr_find(stop_div=False, num_it=200) . . SuggestedLRs(valley=0.013182567432522774) . learn2.fit_one_cycle(5, lr_max=.01) #save_pickle(&#39;learn2_2048.pkl&#39;,learn2) . . . 0.00% [0/5 00:00&lt;00:00] epoch train_loss valid_loss _rmse time . . 47.57% [2388/5020 02:59&lt;03:17 359351840.0000] &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Next Steps . With more time, to improve our model, we would: . Consider different fill strategies for missing data, such as assuming that buildings without a &#39;year_built&#39; are old buildings rather than taking the median. | Separate building 1099 into a separate model, or normalise &#39;meter_reading&#39; per building/meter before inputting it into the model | Explore anomalies such as 0.0 (not missing) for a whole site for a particular day in winter | Check that the test dataset has similar statistical properties to the training/validation set | Experiment with different model layers and sizes | Ensemble with boosted and bagged decision trees | Submit to Kaggle for evaluation against the test set | . &lt;/div&gt; .",
            "url": "https://fastai-energetic-engineering.github.io/ashrae/fastai/kaggle/2021/07/24/_tabular1online-presentation.html",
            "relUrl": "/fastai/kaggle/2021/07/24/_tabular1online-presentation.html",
            "date": " ‚Ä¢ Jul 24, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "ASHRAE Energy Prediction",
            "content": ". The ASHRAE Great Energy Predictor III is a Kaggle competition to predict energy use in buildings. . The training dataset is 3.2GB, containing 20 million energy meter readings for 1449 buildings over a year. Four energy meters are sampled: chilled water, electric, hot water, and steam. Each dependent measurement is given with independent variable metadata for the building and weather. The Kaggle data page shows the available files, with histograms for each variable. . Our goal was to create a model that predicts energy usage per building and per meter in that building, using fast.ai. . We started by downloading the dataset from Kaggle, joining the tables, fixing timestamp inconsistencies and transforming some values into smaller datatypes to save memory. We aimed to work in Colab so memory was precious. We serialised this data preparation to a Parquet file on our Google Drives. . !pip install -Uqq fastbook dtreeviz import fastbook fastbook.setup_book() . . import os import gc import pandas as pd import datetime as dt import seaborn as sns from tqdm.auto import tqdm from pandas.api.types import is_string_dtype, is_numeric_dtype, is_categorical_dtype from fastai.tabular.all import * from sklearn.ensemble import RandomForestRegressor from sklearn.tree import DecisionTreeRegressor from dtreeviz.trees import * from IPython.display import Image, display_svg, SVG from sklearn.metrics import mean_squared_error #mpl.rcParams[&#39;figure.dpi&#39;] = 100 sns.set() pd.options.display.max_rows = 20 pd.options.display.max_columns = 8 . . %cd &#39;/content/gdrive/MyDrive/Colab Notebooks/ashrae&#39; train_valid = pd.read_parquet(&quot;feature_enhanced_train_combined.parquet.snappy&quot;) . . /content/gdrive/MyDrive/Colab Notebooks/ashrae . ## Memory optimization # Original code from https://www.kaggle.com/gemartin/load-data-reduce-memory-usage by @gemartin # Modified to support timestamp type, categorical type # Modified to add option to use float16 from pandas.api.types import is_datetime64_any_dtype as is_datetime from pandas.api.types import is_categorical_dtype def reduce_mem_usage(df, use_float16=False): &quot;&quot;&quot; Iterate through all the columns of a dataframe and modify the data type to reduce memory usage. &quot;&quot;&quot; start_mem = df.memory_usage().sum() / 1024**2 print(&quot;Memory usage of dataframe is {:.2f} MB&quot;.format(start_mem)) for col in df.columns: if is_datetime(df[col]) or is_categorical_dtype(df[col]) or is_string_dtype(df[col]): continue col_type = df[col].dtype if col_type != object: c_min = df[col].min() c_max = df[col].max() if str(col_type)[:3] == &quot;int&quot;: if c_min &gt; np.iinfo(np.int8).min and c_max &lt; np.iinfo(np.int8).max: df[col] = df[col].astype(np.int8) elif c_min &gt; np.iinfo(np.int16).min and c_max &lt; np.iinfo(np.int16).max: df[col] = df[col].astype(np.int16) elif c_min &gt; np.iinfo(np.int32).min and c_max &lt; np.iinfo(np.int32).max: df[col] = df[col].astype(np.int32) elif c_min &gt; np.iinfo(np.int64).min and c_max &lt; np.iinfo(np.int64).max: df[col] = df[col].astype(np.int64) else: if use_float16 and c_min &gt; np.finfo(np.float16).min and c_max &lt; np.finfo(np.float16).max: df[col] = df[col].astype(np.float16) elif c_min &gt; np.finfo(np.float32).min and c_max &lt; np.finfo(np.float32).max: df[col] = df[col].astype(np.float32) else: df[col] = df[col].astype(np.float64) else: df[col] = df[col].astype(&quot;category&quot;) end_mem = df.memory_usage().sum() / 1024**2 print(&quot;Memory usage after optimization is: {:.2f} MB&quot;.format(end_mem)) print(&quot;Decreased by {:.1f}%&quot;.format(100 * (start_mem - end_mem) / start_mem)) return df train_valid = reduce_mem_usage(train_valid, use_float16=True) . . Memory usage of dataframe is 3243.66 MB Memory usage after optimization is: 1189.98 MB Decreased by 63.3% . Exploratory Data Analysis . We started by browsing exploratory data analysis notebooks posted to Kaggle. We learnt that this is a gnarly dataset, with many missing values and outliers. For example, the &#39;meter_reading&#39; dependent variable has mostly small values but also some very large values: . train_valid[[&#39;meter_reading&#39;]].hist() . array([[&lt;matplotlib.axes._subplots.AxesSubplot object at 0x7fc260160550&gt;]], dtype=object) . Exploring further (with credit to https://www.kaggle.com/nroman/eda-for-ashrae) we discover that most of the energy in the dataset is consumed by a single building and single meter, building_id 1099 meter 2. The following plots show daily averaged meter readings for: . the full dataset | just building_id 1099 meter 2 | the dataset with building_id 1099 removed | . It is tempting to model this building independently from the rest of the dataset, to avoid numerical issues and overfitting to this building. . fig, axes = plt.subplots(3,1,figsize=(14, 20), dpi=100) #train_valid[[&#39;timestamp&#39;, &#39;meter_reading&#39;]].set_index(&#39;timestamp&#39;).resample(&#39;H&#39;).mean()[&#39;meter_reading&#39;].plot(ax=axes[1], alpha=0.8, label=&#39;By hour&#39;, color=&#39;tab:blue&#39;).set_ylabel(&#39;Mean meter reading&#39;, fontsize=13); train_valid[[&#39;timestamp&#39;, &#39;meter_reading&#39;]].set_index(&#39;timestamp&#39;).resample(&#39;D&#39;).mean()[&#39;meter_reading&#39;].plot(ax=axes[1], alpha=1, label=&#39;By day&#39;, color=&#39;tab:orange&#39;).set_ylabel(&#39;Mean meter reading&#39;, fontsize=13); #train_valid[(train_valid[&#39;meter&#39;] == 2) &amp; (train_valid[&#39;building_id&#39;] == 1099)][[&#39;timestamp&#39;, &#39;meter_reading&#39;]].set_index(&#39;timestamp&#39;).resample(&#39;H&#39;).mean()[&#39;meter_reading&#39;].plot(ax=axes[0], alpha=0.8, label=&#39;By hour&#39;, color=&#39;tab:blue&#39;).set_ylabel(&#39;Mean meter reading&#39;, fontsize=13); train_valid[(train_valid[&#39;meter&#39;] == 2) &amp; (train_valid[&#39;building_id&#39;] == 1099)][[&#39;timestamp&#39;, &#39;meter_reading&#39;]].set_index(&#39;timestamp&#39;).resample(&#39;D&#39;).mean()[&#39;meter_reading&#39;].plot(ax=axes[0], alpha=1, label=&#39;By day&#39;, color=&#39;tab:orange&#39;).set_ylabel(&#39;Mean meter reading&#39;, fontsize=13); #train_valid[~((train_valid[&#39;meter&#39;] == 2) &amp; (train_valid[&#39;building_id&#39;] == 1099))][[&#39;timestamp&#39;, &#39;meter_reading&#39;]].set_index(&#39;timestamp&#39;).resample(&#39;H&#39;).mean()[&#39;meter_reading&#39;].plot(ax=axes[2], alpha=0.8, label=&#39;By hour&#39;, color=&#39;tab:blue&#39;).set_ylabel(&#39;Mean meter reading&#39;, fontsize=13); train_valid[~((train_valid[&#39;meter&#39;] == 2) &amp; (train_valid[&#39;building_id&#39;] == 1099))][[&#39;timestamp&#39;, &#39;meter_reading&#39;]].set_index(&#39;timestamp&#39;).resample(&#39;D&#39;).mean()[&#39;meter_reading&#39;].plot(ax=axes[2], alpha=1, label=&#39;By day&#39;, color=&#39;tab:orange&#39;).set_ylabel(&#39;Mean meter reading&#39;, fontsize=13); axes[0].set_title(&#39;Full dataset&#39;, fontsize=13); axes[1].set_title(&#39;building_id==1099 and meter==2&#39;, fontsize=13); axes[2].set_title(&#39;building_id 1099 excluded&#39;, fontsize=13); plt.subplots_adjust(hspace=0.45) . . Splitting into Training and Validation sets . Given that we have a year&#39;s worth of time series data, we investigated several methods for splitting into training and validation sets. We ruled out random splits but considered periodic splits such as taking every 4th week for validation. In the end, we opted for a simple 2-fold time-based cross-validation, taking the first 6 months of data to train one model and the second 6 months for a second model, averaging predictions from each. . Data Transforms . We started with the default trio of fast.ai transforms: Categorify, FillMissing, Normalize. We experimented with omitting each and saw our model performance degrade. We also experimented with dropping rows that were missing the key independent variables, since there aren&#39;t many such rows. This approach gave good performance so we would consider this when tuning our model. . Feature Engineering . We engineered some additional time features from the &#39;timestamp&#39; variable, namely: . year_fraction | week_fraction | day_fraction We found through feature clustering that &#39;year_fraction&#39; was highly correlated with &#39;air_temperature&#39; which makes sense because air temperature is a good proxy for month of the year. We omitted &#39;year_fraction&#39; to simplify the model. | . # Get a subset of the data, a few buildings and all meters #train = train_valid[(train_valid[&#39;building_id&#39;]&gt;=1126)] train_valid[&#39;meter_reading&#39;] = np.log1p(train_valid[&#39;meter_reading&#39;]) # convert target to log train = train_valid del train_valid gc.collect() . . 150 . The code below shows how we created the DataLoaders, using only a subset of the most important features: . # 2-fold cross-validation. Build 2 models, each with half the year as training data splits1=MaskSplitter(train[&#39;timestamp&#39;] &gt;= &#39;2016-07-01&#39;)(range_of(train)) cat_names = [&#39;building_id&#39;, &#39;meter&#39;, &#39;site_id&#39;, &#39;primary_use&#39;] cont_names = [&#39;air_temperature&#39;, &#39;dew_temperature&#39;, &#39;square_feet&#39;, &#39;week_fraction&#39;, &#39;day_fraction&#39;] procs = [Categorify, FillMissing, Normalize] to1 = TabularPandas(train, procs=procs, cat_names = cat_names, cont_names = cont_names, y_names=&#39;meter_reading&#39;, splits=splits1) dls1 = to1.dataloaders(bs=1024) del splits1#, to1 gc.collect() . 20 . splits2=MaskSplitter(train[&#39;timestamp&#39;] &lt; &#39;2016-07-01&#39;)(range_of(train)) to2 = TabularPandas(train, procs=procs, cat_names = cat_names, cont_names = cont_names, y_names=&#39;meter_reading&#39;, splits=splits2) dls2 = to2.dataloaders(bs=1024) del splits2, to2 gc.collect() . 486 . Feature Importance . We trained a scikit-learn DecisionTreeRegressor to compare against our neural network and to assess feature importance. . We discovered that the most significant features influencing energy usage for a particular building are: . &#39;square_feet&#39; representing the size of the building | &#39;air_temperature&#39; which is unsurprising | &#39;primary_use&#39; surprisingly, whether a building is used for education, office, retail etc has a large influence on energy use | &#39;dew_temperature&#39; is not as correlated with air temperature as you would expect | &#39;day_fraction&#39; meaning that energy is consumed at regular times every day | &#39;week_fraction&#39; indicating that energy use changes through the week | . # Train a fast.ai decision tree m = DecisionTreeRegressor(min_samples_leaf=2000) m.fit(to1.train.xs, to1.train.y); . . def rf_feat_importance(m, df): return pd.DataFrame({&#39;feature&#39;:df.columns, &#39;importance&#39;:m.feature_importances_} ).sort_values(&#39;importance&#39;, ascending=False) fi = rf_feat_importance(m, to1.train.xs) fi[:10] . . feature importance . 8 square_feet | 0.383488 | . 0 building_id | 0.244590 | . 1 meter | 0.135766 | . 6 air_temperature | 0.098082 | . 2 site_id | 0.047038 | . 3 primary_use | 0.046512 | . 7 dew_temperature | 0.032303 | . 10 day_fraction | 0.009538 | . 9 week_fraction | 0.002682 | . 4 air_temperature_na | 0.000000 | . print(&quot;Training set RMSLE: &quot; + str(mean_squared_error(to1.train.y, m.predict(to1.train.xs), squared=False))) print(&quot;Validation set RMSLE: &quot; + str(mean_squared_error(to1.valid.y, m.predict(to1.valid.xs), squared=False))) building=2 meter=2 #plt.scatter(valid_xs[(valid_xs[&#39;building_id&#39;]==building) &amp; (valid_xs[&#39;meter&#39;]==meter)][&#39;air_temperature&#39;], valid_y.loc[valid_xs[(valid_xs[&#39;building_id&#39;]==building) &amp; (valid_xs[&#39;meter&#39;]==meter)].index]) #plt.scatter(valid_xs[(valid_xs[&#39;building_id&#39;]==building) &amp; (valid_xs[&#39;meter&#39;]==meter)][&#39;air_temperature&#39;], pd.DataFrame(valid_y_pred, index=valid_y.index).loc[valid_xs[(valid_xs[&#39;building_id&#39;]==building) &amp; (valid_xs[&#39;meter&#39;]==meter)].index], color=&#39;orange&#39;) . . Training set RMSLE: 1.0176237141131312 Validation set RMSLE: 1.4695399113467222 . Learning Rates . We tried using lr_find() to choose a learning rate. However, the curve is upside down. . learn1 = tabular_learner(dls1, metrics=rmse) #learn1.summary() # Exploring learning rates learn1.lr_find(stop_div=False, num_it=200) . SuggestedLRs(valley=0.0027542286552488804) . learn2 = tabular_learner(dls2, metrics=rmse) . Model Training . So, we trained a bunch of models, experimenting with all the hyperparameters discussed, including: . Features | Batch sizes | Learning rates | Weight decay | Number of buildings included | . learn1.fit_one_cycle(5, lr_max=.00001) . epoch train_loss valid_loss _rmse time . 0 | 15.373385 | 16.623022 | 4.077135 | 07:13 | . 1 | 3.959580 | 5.197935 | 2.279897 | 07:34 | . 2 | 0.958351 | 1.994714 | 1.412344 | 07:08 | . 3 | 0.927223 | 1.965125 | 1.401830 | 07:19 | . 4 | 0.911948 | 1.951616 | 1.397004 | 07:12 | . learn2.fit_one_cycle(5, lr_max=.00001) . . # Pickling didn&#39;t work due to an unspecified google drive error #outfile = open(&#39;learn1.pkl&#39;,&#39;wb&#39;) #save_pickle(outfile, learn1) . . #infile = open(&#39;learn1.pkl&#39;,&#39;rb&#39;) #learn1 = load_pickle(infile) . . Metrics . The competition stated that that chosen metric was root mean squared log error: RMSLE=sqrt(1/ùëõ‚àë(log(ùëù+1)‚àílog(ùëé+1))**2) . fast.ai doesn&#39;t have this metric but the metric can be achieved by taking the log of the dependent variable, &#39;meter_reading&#39; and then using root mean squared error (RMSE). . We found that using the log of &#39;meter_reading&#39; had the added bonus of much better performance and more stable training. This is because &#39;meter_reading&#39; had a large dynamic range, with some small values and some very large values. Taking the log is a form of normalisation, which is more numerically stable. . learn1.show_results() from sklearn.metrics import mean_squared_error y_pred,y_targs = learn.get_preds(ds_idx=0) valid_y_pred,valid_y_targs = learn.get_preds() print(&quot;Training set RMSLE: &quot; + str(mean_squared_error(y_pred.detach().numpy(), to1.train.y, squared=False))) print(&quot;Validation set RMSLE: &quot; + str(mean_squared_error(valid_y_pred.detach().numpy(), to1.valid.y, squared=False))) building=2 meter=2 #plt.scatter(valid_xs[(valid_xs[&#39;building_id&#39;]==building) &amp; (valid_xs[&#39;meter&#39;]==meter)][&#39;air_temperature&#39;], valid_y.loc[valid_xs[(valid_xs[&#39;building_id&#39;]==building) &amp; (valid_xs[&#39;meter&#39;]==meter)].index]) #plt.scatter(valid_xs[(valid_xs[&#39;building_id&#39;]==building) &amp; (valid_xs[&#39;meter&#39;]==meter)][&#39;air_temperature&#39;], pd.DataFrame(valid_y_pred, index=valid_y.index).loc[valid_xs[(valid_xs[&#39;building_id&#39;]==building) &amp; (valid_xs[&#39;meter&#39;]==meter)].index], color=&#39;orange&#39;) . building_id meter site_id primary_use air_temperature_na dew_temperature_na air_temperature dew_temperature square_feet week_fraction day_fraction meter_reading meter_reading_pred . 0 1421.0 | 3.0 | 16.0 | 1.0 | 1.0 | 1.0 | -0.993731 | -0.687863 | 0.627916 | -0.881094 | 0.789366 | 8.051306 | 8.185560 | . 1 1123.0 | 3.0 | 14.0 | 7.0 | 1.0 | 1.0 | -0.631852 | -0.241622 | 0.220548 | 0.529391 | 0.213382 | 6.476108 | 1.767113 | . 2 35.0 | 1.0 | 1.0 | 5.0 | 1.0 | 1.0 | 0.138681 | 0.773319 | -0.554065 | -0.984849 | 0.070228 | 5.498250 | -0.146082 | . 3 1214.0 | 2.0 | 14.0 | 10.0 | 1.0 | 1.0 | -0.019414 | 0.154288 | 2.347358 | 1.337326 | -1.079634 | 3.573828 | 3.585569 | . 4 1050.0 | 1.0 | 13.0 | 1.0 | 1.0 | 1.0 | 0.454872 | 0.488771 | -0.294969 | 0.570213 | 0.501374 | 4.512150 | 4.103727 | . 5 96.0 | 2.0 | 1.0 | 5.0 | 1.0 | 1.0 | 0.036427 | -0.525625 | 0.801990 | -1.731126 | -1.654355 | 7.001674 | 3.674653 | . 6 791.0 | 3.0 | 8.0 | 1.0 | 1.0 | 1.0 | 0.583959 | 1.442284 | 5.630443 | 1.357737 | -0.935638 | 0.000000 | -0.493652 | . 7 830.0 | 1.0 | 9.0 | 10.0 | 1.0 | 1.0 | 1.225042 | 1.340829 | -0.850327 | -0.922341 | 0.501374 | 3.266076 | 2.952885 | . 8 217.0 | 1.0 | 3.0 | 1.0 | 1.0 | 1.0 | 1.011831 | -0.069030 | 0.254535 | -0.839846 | 1.075674 | 5.334215 | 5.187250 | . We achieved a validation set RMSLE of 4.51, compared to validation set RMSLE of 2.02 for our DecisionTreeRegressor. . The winning Kaggle entry scored RMSLE of 1.23. Assuming that our cross-validated sets are representative of the test set (a big assumption!) this puts us around 3000th on the Kaggle leaderboard of 3500 entries. That&#39;s not great but not embarrasing either. . from sklearn.metrics import mean_squared_error #collapse if &#39;to1&#39; in locals(): del to1 if &#39;dls1&#39; in locals(): del dls1 if &#39;to2&#39; in locals(): del to2 if &#39;dls2&#39; in locals(): del dls2 # y_targs2 is the same as to2.train.y # valid_y_targs2 is the same as to2.valid.y y_pred1,y_targs1 = learn1.get_preds(ds_idx=0) # training set predictions valid_y_pred1,valid_y_targs1 = learn1.get_preds() # validation set predictions print(&quot;Model 1 training set RMSLE: &quot; + str(mean_squared_error(y_pred1.detach().numpy(), y_targs1, squared=False))) print(&quot;Model 1 Validation set RMSLE: &quot; + str(mean_squared_error(valid_y_pred1.detach().numpy(), valid_y_targs1, squared=False))) y_pred2,y_targs2 = learn2.get_preds(ds_idx=0) # training set predictions valid_y_pred2,valid_y_targs2 = learn2.get_preds() # validation set predictions print(&quot;Model 2 training set RMSLE: &quot; + str(mean_squared_error(y_pred2.detach().numpy(), y_targs2, squared=False))) print(&quot;Model 2 Validation set RMSLE: &quot; + str(mean_squared_error(valid_y_pred2.detach().numpy(), valid_y_targs2, squared=False))) . . Training set RMSLE: 4.786435 Validation set RMSLE: 4.643143 . Next Steps . With more time, to improve our model, we would: . Consider different fill strategies for missing data, such as assuming that buildings without a &#39;year_built&#39; are old buildings rather than taking the median. | Separate building 1099 into a separate model, or normalise &#39;meter_reading&#39; per building/meter before inputting it into the model | Explore anomalies such as 0.0 (not missing) for a whole site for a particular day in winter | Check that the test dataset has similar statistical properties to the training/validation set | Experiment with different model layers and sizes | Ensemble with boosted and bagged decision trees | Submit to Kaggle for evaluation against the test set | .",
            "url": "https://fastai-energetic-engineering.github.io/ashrae/fastai/kaggle/2021/07/23/tabular1online-presentation.html",
            "relUrl": "/fastai/kaggle/2021/07/23/tabular1online-presentation.html",
            "date": " ‚Ä¢ Jul 23, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Getting Kaggle Data for ASHRAE Energy Prediction",
            "content": ". !pip install -Uqq fastbook import fastbook fastbook.setup_book() . . |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 727kB 8.1MB/s |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 194kB 35.5MB/s |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.2MB 36.4MB/s |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 51kB 9.3MB/s |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 61kB 10.1MB/s |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 61kB 10.9MB/s Mounted at /content/gdrive . from fastbook import * import os from google.colab import files import pandas as pd import datetime . . This notebook demonstrates how I downloaded the ASHRAE Energy Prediction Data from Kaggle. . First, we need to install the Kaggle API. . !pip install kaggle --upgrade -q . I will download the data into a folder in my google drive. First, I will set my home directory. . p = Path(&#39;drive/MyDrive/Colab Notebooks/ashrae/&#39;) os.chdir(p) # change directory . We need to download Kaggle API token and then put the .json file in .kaggle folder. We can upload the key directly from colab. . files.upload() # use this to upload your API json key !mkdir ~/.kaggle # create folder !cp kaggle.json ~/.kaggle/ # move the key into the folder !chmod 600 ~/.kaggle/kaggle.json # change permissions of the file . We can finally download the file! . os.chdir(&#39;data&#39;) # move to data folder !kaggle competitions download -c ashrae-energy-prediction . for item in os.listdir(): # for every item in the folder if item.endswith(&#39;.zip&#39;): # check if it is a .zip file file_extract(item) # if it is, then extract file os.remove(item) # and then remove the .zip . os.chdir(&quot;..&quot;) # return to initial folder . Joining Tables . Our training data comprised of three tables: . building_metadata.csv | weather_train.csv | train.csv | . We need to join the tables. First, let&#39;s see what&#39;s in the tables. . building = pd.read_csv(&#39;data/building_metadata.csv&#39;) weather = pd.read_csv(&#39;data/weather_train.csv&#39;) train = pd.read_csv(&#39;data/train.csv&#39;) . building contains the buildings&#39; metadata. . building.head() . site_id building_id primary_use square_feet year_built floor_count . 0 0 | 0 | Education | 7432 | 2008.0 | NaN | . 1 0 | 1 | Education | 2720 | 2004.0 | NaN | . 2 0 | 2 | Education | 5376 | 1991.0 | NaN | . 3 0 | 3 | Education | 23685 | 2002.0 | NaN | . 4 0 | 4 | Education | 116607 | 1975.0 | NaN | . site_id - Foreign key for the weather files. | building_id - Foreign key for training.csv | primary_use - Indicator of the primary category of activities for the building based on EnergyStar property type definitions | square_feet - Gross floor area of the building | year_built - Year building was opened | floor_count - Number of floors of the building | . weather contains weather data from the closest meteorological station. . weather.head() . site_id timestamp air_temperature cloud_coverage dew_temperature precip_depth_1_hr sea_level_pressure wind_direction wind_speed . 0 0 | 2016-01-01 00:00:00 | 25.0 | 6.0 | 20.0 | NaN | 1019.7 | 0.0 | 0.0 | . 1 0 | 2016-01-01 01:00:00 | 24.4 | NaN | 21.1 | -1.0 | 1020.2 | 70.0 | 1.5 | . 2 0 | 2016-01-01 02:00:00 | 22.8 | 2.0 | 21.1 | 0.0 | 1020.2 | 0.0 | 0.0 | . 3 0 | 2016-01-01 03:00:00 | 21.1 | 2.0 | 20.6 | 0.0 | 1020.1 | 0.0 | 0.0 | . 4 0 | 2016-01-01 04:00:00 | 20.0 | 2.0 | 20.0 | -1.0 | 1020.0 | 250.0 | 2.6 | . site_id | air_temperature - Degrees Celsius | cloud_coverage - Portion of the sky covered in clouds, in oktas | dew_temperature - Degrees Celsius | precip_depth_1_hr - Millimeters | sea_level_pressure - Millibar/hectopascals | wind_direction - Compass direction (0-360) | wind_speed - Meters per second | . Finally, train contains the target variable, meter reading, which represents energy consumption in kWh. . train.head() . building_id meter timestamp meter_reading . 0 0 | 0 | 2016-01-01 00:00:00 | 0.0 | . 1 1 | 0 | 2016-01-01 00:00:00 | 0.0 | . 2 2 | 0 | 2016-01-01 00:00:00 | 0.0 | . 3 3 | 0 | 2016-01-01 00:00:00 | 0.0 | . 4 4 | 0 | 2016-01-01 00:00:00 | 0.0 | . building_id - Foreign key for the building metadata. | meter - The meter id code. Read as {0: electricity, 1: chilledwater, 2: steam, 3: hotwater}. Not every building has all meter types. | timestamp - When the measurement was taken | meter_reading - The target variable. Energy consumption in kWh (or equivalent). | . Apparently there was some issues regarding the timestamps, as noted by this post. The timestamp in the weather and meter reading table were in GMT and local time, respectively. We have to keep this in mind before merging the tables. . Here I wrote a function that can prepare train and test data accordingly. . def prepare_data(type=&#39;train&#39;): assert type in [&#39;train&#39;, &#39;test&#39;] # read data building = pd.read_csv(&#39;data/building_metadata.csv&#39;) weather = pd.read_csv(f&#39;data/weather_{type}.csv&#39;) data = pd.read_csv(f&#39;data/{type}.csv&#39;) # convert datetime data[&#39;timestamp&#39;] = pd.to_datetime(data[&#39;timestamp&#39;]) # adjust timestamp timediff = {0:4,1:0,2:7,3:4,4:7,5:0,6:4,7:4,8:4,9:5,10:7,11:4,12:0,13:5,14:4,15:4} weather[&#39;time_diff&#39;]= weather[&#39;site_id&#39;].map(timediff) weather[&#39;time_diff&#39;] = weather[&#39;time_diff&#39;].apply(lambda x: datetime.timedelta(hours=x)) weather[&#39;timestamp_gmt&#39;] = pd.to_datetime(weather[&#39;timestamp&#39;]) weather[&#39;timestamp&#39;] = weather[&#39;timestamp_gmt&#39;] - weather[&#39;time_diff&#39;] # merge table data = data.merge(building, on=&#39;building_id&#39;, how=&#39;left&#39;) data = data.merge(weather, on=[&#39;site_id&#39;,&#39;timestamp&#39;], how=&#39;left&#39;) return data . Let&#39;s try this function out! . prepare_data(&#39;train&#39;).head() . building_id meter timestamp meter_reading site_id primary_use square_feet year_built floor_count air_temperature cloud_coverage dew_temperature precip_depth_1_hr sea_level_pressure wind_direction wind_speed time_diff timestamp_gmt . 0 0 | 0 | 2016-01-01 | 0.0 | 0 | Education | 7432 | 2008.0 | NaN | 20.0 | 2.0 | 20.0 | -1.0 | 1020.0 | 250.0 | 2.6 | 0 days 04:00:00 | 2016-01-01 04:00:00 | . 1 1 | 0 | 2016-01-01 | 0.0 | 0 | Education | 2720 | 2004.0 | NaN | 20.0 | 2.0 | 20.0 | -1.0 | 1020.0 | 250.0 | 2.6 | 0 days 04:00:00 | 2016-01-01 04:00:00 | . 2 2 | 0 | 2016-01-01 | 0.0 | 0 | Education | 5376 | 1991.0 | NaN | 20.0 | 2.0 | 20.0 | -1.0 | 1020.0 | 250.0 | 2.6 | 0 days 04:00:00 | 2016-01-01 04:00:00 | . 3 3 | 0 | 2016-01-01 | 0.0 | 0 | Education | 23685 | 2002.0 | NaN | 20.0 | 2.0 | 20.0 | -1.0 | 1020.0 | 250.0 | 2.6 | 0 days 04:00:00 | 2016-01-01 04:00:00 | . 4 4 | 0 | 2016-01-01 | 0.0 | 0 | Education | 116607 | 1975.0 | NaN | 20.0 | 2.0 | 20.0 | -1.0 | 1020.0 | 250.0 | 2.6 | 0 days 04:00:00 | 2016-01-01 04:00:00 | . That&#39;s it! In the next blogpost, I will show how to load this data into FastAI&#39;s dataloaders. .",
            "url": "https://fastai-energetic-engineering.github.io/ashrae/kaggle/preprocessing/2021/06/27/Getting-ASHRAE-Energy-Prediction-Data-from-Kaggle.html",
            "relUrl": "/kaggle/preprocessing/2021/06/27/Getting-ASHRAE-Energy-Prediction-Data-from-Kaggle.html",
            "date": " ‚Ä¢ Jun 27, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Us",
          "content": "Team Energetic Engineering was formed for the Queensland AI‚Äôs FastAI course group project. We focus on tabular data analysis using the ASHRAE - Great Energy Predictor dataset from Kaggle. We are a group of energetic learners comprising (in alphabetical order): . Mikhael Manurung | Oluwakayode Olamoyegun | Owen Lamont | Roger Butler | . As part of our individual learning journeys, we each will be contributing notebooks in this site. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats.¬†&#8617; . |",
          "url": "https://fastai-energetic-engineering.github.io/ashrae/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ ‚Äúsitemap.xml‚Äù | absolute_url }} | .",
          "url": "https://fastai-energetic-engineering.github.io/ashrae/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}