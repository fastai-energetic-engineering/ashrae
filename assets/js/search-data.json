{
  
    
        "post0": {
            "title": "ASHRAE Energy Prediction",
            "content": ". The ASHRAE Great Energy Predictor III is a Kaggle competition to predict energy use in buildings. . The training dataset is 3.2GB, containing 20 million energy meter readings for 1449 buildings over a year. Four energy meters are sampled: chilled water, electric, hot water, and steam. Each dependent measurement is given with independent variable metadata for the building and weather. The Kaggle data page shows the available files, with histograms for each variable. . Our goal was to create a model that predicts energy usage per building and per meter in that building, using fast.ai. . We started by downloading the dataset from Kaggle, joining the tables, fixing timestamp inconsistencies and transforming some values into smaller datatypes to save memory. We aimed to work in Colab so memory was precious. . !pip install -Uqq fastbook dtreeviz import fastbook fastbook.setup_book() . . import os import gc import pandas as pd import datetime as dt from tqdm.auto import tqdm from pandas.api.types import is_string_dtype, is_numeric_dtype, is_categorical_dtype from fastai.tabular.all import * from sklearn.ensemble import RandomForestRegressor from sklearn.tree import DecisionTreeRegressor from dtreeviz.trees import * from IPython.display import Image, display_svg, SVG pd.options.display.max_rows = 20 pd.options.display.max_columns = 8 . . %cd &#39;/content/gdrive/MyDrive/Colab Notebooks/ashrae&#39; train_valid = pd.read_parquet(&quot;feature_enhanced_train_combined.parquet.snappy&quot;) . . ## Memory optimization # Original code from https://www.kaggle.com/gemartin/load-data-reduce-memory-usage by @gemartin # Modified to support timestamp type, categorical type # Modified to add option to use float16 from pandas.api.types import is_datetime64_any_dtype as is_datetime from pandas.api.types import is_categorical_dtype def reduce_mem_usage(df, use_float16=False): &quot;&quot;&quot; Iterate through all the columns of a dataframe and modify the data type to reduce memory usage. &quot;&quot;&quot; start_mem = df.memory_usage().sum() / 1024**2 print(&quot;Memory usage of dataframe is {:.2f} MB&quot;.format(start_mem)) for col in df.columns: if is_datetime(df[col]) or is_categorical_dtype(df[col]) or is_string_dtype(df[col]): continue col_type = df[col].dtype if col_type != object: c_min = df[col].min() c_max = df[col].max() if str(col_type)[:3] == &quot;int&quot;: if c_min &gt; np.iinfo(np.int8).min and c_max &lt; np.iinfo(np.int8).max: df[col] = df[col].astype(np.int8) elif c_min &gt; np.iinfo(np.int16).min and c_max &lt; np.iinfo(np.int16).max: df[col] = df[col].astype(np.int16) elif c_min &gt; np.iinfo(np.int32).min and c_max &lt; np.iinfo(np.int32).max: df[col] = df[col].astype(np.int32) elif c_min &gt; np.iinfo(np.int64).min and c_max &lt; np.iinfo(np.int64).max: df[col] = df[col].astype(np.int64) else: if use_float16 and c_min &gt; np.finfo(np.float16).min and c_max &lt; np.finfo(np.float16).max: df[col] = df[col].astype(np.float16) elif c_min &gt; np.finfo(np.float32).min and c_max &lt; np.finfo(np.float32).max: df[col] = df[col].astype(np.float32) else: df[col] = df[col].astype(np.float64) else: df[col] = df[col].astype(&quot;category&quot;) end_mem = df.memory_usage().sum() / 1024**2 print(&quot;Memory usage after optimization is: {:.2f} MB&quot;.format(end_mem)) print(&quot;Decreased by {:.1f}%&quot;.format(100 * (start_mem - end_mem) / start_mem)) return df train_valid = reduce_mem_usage(train_valid, use_float16=True) . . We started by browsing exploratory data analysis notebooks posted to Kaggle. We learnt that this is a gnarly dataset, with many missing values and outliers. For example, the &#39;meter_reading&#39; dependent variable has mostly small values but also some very large values: . train_valid[[&#39;meter_reading&#39;]].hist() .",
            "url": "https://fastai-energetic-engineering.github.io/ashrae/fastai/kaggle/2021/07/23/tabular1online-presentation.html",
            "relUrl": "/fastai/kaggle/2021/07/23/tabular1online-presentation.html",
            "date": " ‚Ä¢ Jul 23, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "ASHRAE Energy Prediction",
            "content": ". The ASHRAE Great Energy Predictor III is a Kaggle competition to predict energy use in buildings. . The training dataset is 3.2GB, containing 20 million energy meter readings for 1449 buildings over a year. Four energy meters are sampled: chilled water, electric, hot water, and steam. Each dependent measurement is given with independent variable metadata for the building and weather. The Kaggle data page shows the available files, with histograms for each variable. . Our goal was to create a model that predicts energy usage per building and per meter in that building, using fast.ai. . We started by downloading the dataset from Kaggle, joining the tables, fixing timestamp inconsistencies and transforming some values into smaller datatypes to save memory. We aimed to work in Colab so memory was precious. We serialised this data preparation to a Parquet file on our Google Drives. . !pip install -Uqq fastbook dtreeviz import fastbook fastbook.setup_book() . . import os import gc import pandas as pd import datetime as dt import seaborn as sns from tqdm.auto import tqdm from pandas.api.types import is_string_dtype, is_numeric_dtype, is_categorical_dtype from fastai.tabular.all import * from sklearn.ensemble import RandomForestRegressor from sklearn.tree import DecisionTreeRegressor from dtreeviz.trees import * from IPython.display import Image, display_svg, SVG from sklearn.metrics import mean_squared_log_error #mpl.rcParams[&#39;figure.dpi&#39;] = 100 sns.set() pd.options.display.max_rows = 20 pd.options.display.max_columns = 8 . . %cd &#39;/content/gdrive/MyDrive/Colab Notebooks/ashrae&#39; train_valid = pd.read_parquet(&quot;feature_enhanced_train_combined.parquet.snappy&quot;) . . /content/gdrive/MyDrive/Colab Notebooks/ashrae . ## Memory optimization # Original code from https://www.kaggle.com/gemartin/load-data-reduce-memory-usage by @gemartin # Modified to support timestamp type, categorical type # Modified to add option to use float16 from pandas.api.types import is_datetime64_any_dtype as is_datetime from pandas.api.types import is_categorical_dtype def reduce_mem_usage(df, use_float16=False): &quot;&quot;&quot; Iterate through all the columns of a dataframe and modify the data type to reduce memory usage. &quot;&quot;&quot; start_mem = df.memory_usage().sum() / 1024**2 print(&quot;Memory usage of dataframe is {:.2f} MB&quot;.format(start_mem)) for col in df.columns: if is_datetime(df[col]) or is_categorical_dtype(df[col]) or is_string_dtype(df[col]): continue col_type = df[col].dtype if col_type != object: c_min = df[col].min() c_max = df[col].max() if str(col_type)[:3] == &quot;int&quot;: if c_min &gt; np.iinfo(np.int8).min and c_max &lt; np.iinfo(np.int8).max: df[col] = df[col].astype(np.int8) elif c_min &gt; np.iinfo(np.int16).min and c_max &lt; np.iinfo(np.int16).max: df[col] = df[col].astype(np.int16) elif c_min &gt; np.iinfo(np.int32).min and c_max &lt; np.iinfo(np.int32).max: df[col] = df[col].astype(np.int32) elif c_min &gt; np.iinfo(np.int64).min and c_max &lt; np.iinfo(np.int64).max: df[col] = df[col].astype(np.int64) else: if use_float16 and c_min &gt; np.finfo(np.float16).min and c_max &lt; np.finfo(np.float16).max: df[col] = df[col].astype(np.float16) elif c_min &gt; np.finfo(np.float32).min and c_max &lt; np.finfo(np.float32).max: df[col] = df[col].astype(np.float32) else: df[col] = df[col].astype(np.float64) else: df[col] = df[col].astype(&quot;category&quot;) end_mem = df.memory_usage().sum() / 1024**2 print(&quot;Memory usage after optimization is: {:.2f} MB&quot;.format(end_mem)) print(&quot;Decreased by {:.1f}%&quot;.format(100 * (start_mem - end_mem) / start_mem)) return df train_valid = reduce_mem_usage(train_valid, use_float16=True) . . Memory usage of dataframe is 3243.66 MB Memory usage after optimization is: 1189.98 MB Decreased by 63.3% . Exploratory Data Analysis . We started by browsing exploratory data analysis notebooks posted to Kaggle. We learnt that this is a gnarly dataset, with many missing values and outliers. For example, the &#39;meter_reading&#39; dependent variable has mostly small values but also some very large values: . train_valid[[&#39;meter_reading&#39;]].hist() . array([[&lt;matplotlib.axes._subplots.AxesSubplot object at 0x7fa73db7dad0&gt;]], dtype=object) . Exploring further (with credit to https://www.kaggle.com/nroman/eda-for-ashrae) we discover that most of the energy in the dataset is consumed by a single building and single meter, building_id 1099 meter 2. The following plots show daily averaged meter readings for: . the full dataset | just building_id 1099 meter 2 | the dataset with building_id 1099 removed | . It is tempting to model this building independently from the rest of the dataset, to avoid numerical issues and overfitting to this building. . fig, axes = plt.subplots(3,1,figsize=(14, 20), dpi=100) #train_valid[[&#39;timestamp&#39;, &#39;meter_reading&#39;]].set_index(&#39;timestamp&#39;).resample(&#39;H&#39;).mean()[&#39;meter_reading&#39;].plot(ax=axes[1], alpha=0.8, label=&#39;By hour&#39;, color=&#39;tab:blue&#39;).set_ylabel(&#39;Mean meter reading&#39;, fontsize=13); train_valid[[&#39;timestamp&#39;, &#39;meter_reading&#39;]].set_index(&#39;timestamp&#39;).resample(&#39;D&#39;).mean()[&#39;meter_reading&#39;].plot(ax=axes[1], alpha=1, label=&#39;By day&#39;, color=&#39;tab:orange&#39;).set_ylabel(&#39;Mean meter reading&#39;, fontsize=13); #train_valid[(train_valid[&#39;meter&#39;] == 2) &amp; (train_valid[&#39;building_id&#39;] == 1099)][[&#39;timestamp&#39;, &#39;meter_reading&#39;]].set_index(&#39;timestamp&#39;).resample(&#39;H&#39;).mean()[&#39;meter_reading&#39;].plot(ax=axes[0], alpha=0.8, label=&#39;By hour&#39;, color=&#39;tab:blue&#39;).set_ylabel(&#39;Mean meter reading&#39;, fontsize=13); train_valid[(train_valid[&#39;meter&#39;] == 2) &amp; (train_valid[&#39;building_id&#39;] == 1099)][[&#39;timestamp&#39;, &#39;meter_reading&#39;]].set_index(&#39;timestamp&#39;).resample(&#39;D&#39;).mean()[&#39;meter_reading&#39;].plot(ax=axes[0], alpha=1, label=&#39;By day&#39;, color=&#39;tab:orange&#39;).set_ylabel(&#39;Mean meter reading&#39;, fontsize=13); #train_valid[~((train_valid[&#39;meter&#39;] == 2) &amp; (train_valid[&#39;building_id&#39;] == 1099))][[&#39;timestamp&#39;, &#39;meter_reading&#39;]].set_index(&#39;timestamp&#39;).resample(&#39;H&#39;).mean()[&#39;meter_reading&#39;].plot(ax=axes[2], alpha=0.8, label=&#39;By hour&#39;, color=&#39;tab:blue&#39;).set_ylabel(&#39;Mean meter reading&#39;, fontsize=13); train_valid[~((train_valid[&#39;meter&#39;] == 2) &amp; (train_valid[&#39;building_id&#39;] == 1099))][[&#39;timestamp&#39;, &#39;meter_reading&#39;]].set_index(&#39;timestamp&#39;).resample(&#39;D&#39;).mean()[&#39;meter_reading&#39;].plot(ax=axes[2], alpha=1, label=&#39;By day&#39;, color=&#39;tab:orange&#39;).set_ylabel(&#39;Mean meter reading&#39;, fontsize=13); axes[0].set_title(&#39;Full dataset&#39;, fontsize=13); axes[1].set_title(&#39;building_id==1099 and meter==2&#39;, fontsize=13); axes[2].set_title(&#39;building_id 1099 excluded&#39;, fontsize=13); plt.subplots_adjust(hspace=0.45) . Splitting into Training and Validation sets . Given that we have a year&#39;s worth of time series data, we investigated several methods for splitting into training and validation sets. We ruled out random splits but considered periodic splits such as taking every 4th week for validation. In the end, we opted for a simple 2-fold time-based cross-validation, taking the first 6 months of data to train one model and the second 6 months for a second model, averaging predictions from each. . Data Transforms . We started with the default trio of fast.ai transforms: Categorify, FillMissing, Normalize. We experimented with omitting each and saw our model performance degrade. We also experimented with dropping rows that were missing the key independent variables, since there aren&#39;t many such rows. This approach gave good performance so we would consider this when tuning our model. . Feature engineering . We engineered some additional time features from the &#39;timestamp&#39; variable, namely: . year_fraction | week_fraction | day_fraction We found through feature clustering that &#39;year_fraction&#39; was highly correlated with &#39;air_temperature&#39; which makes sense because air temperature is a good proxy for month of the year. We omitted &#39;year_fraction&#39; to simplify the model. | . # Get a subset of the data, a few buildings and all meters train = train_valid[(train_valid[&#39;building_id&#39;]&gt;=1126)] #del train_valid gc.collect() . . 80137 . The code below shows how we created the DataLoaders, using only a subset of the most important features: . # 2-fold cross-validation. Build 2 models, each with half the year as training data splits1=MaskSplitter(train[&#39;timestamp&#39;] &gt;= &#39;2016-07-01&#39;)(range_of(train)) cat_names = [&#39;building_id&#39;, &#39;meter&#39;, &#39;site_id&#39;, &#39;primary_use&#39;] cont_names = [&#39;air_temperature&#39;, &#39;dew_temperature&#39;, &#39;square_feet&#39;, &#39;week_fraction&#39;, &#39;day_fraction&#39;] procs = [Categorify, FillMissing, Normalize] to1 = TabularPandas(train, procs=procs, cat_names = cat_names, cont_names = cont_names, y_names=&#39;meter_reading&#39;, splits=splits1) dls1 = to1.dataloaders(bs=1024) . Feature Importance . We trained a scikit-learn DecisionTreeRegressor to compare against our neural network and to assess feature importance. . We discovered that the most significant features influencing energy usage for a particular building are: . &#39;primary_use&#39; surprisingly, whether a building is used for education, office, retail etc has a large influence on energy use | &#39;square_feet&#39; representing the size of the building | &#39;air_temperature&#39; which is unsurprising | &#39;dew_temperature&#39; is not as correlated with air temperature as you would expect | &#39;day_fraction&#39; meaning that energy is consumed at regular times every day | . # Train a fast.ai decision tree xs,y = to1.train.xs,to1.train.y valid_xs,valid_y = to1.valid.xs,to1.valid.y m = DecisionTreeRegressor(min_samples_leaf=1000) m.fit(xs, y); . . def rf_feat_importance(m, df): return pd.DataFrame({&#39;feature&#39;:df.columns, &#39;importance&#39;:m.feature_importances_} ).sort_values(&#39;importance&#39;, ascending=False) fi = rf_feat_importance(m, xs) fi[:10] . . feature importance . 1 meter | 0.303746 | . 3 primary_use | 0.280424 | . 8 square_feet | 0.209023 | . 6 air_temperature | 0.161648 | . 0 building_id | 0.039026 | . 2 site_id | 0.003520 | . 7 dew_temperature | 0.001692 | . 10 day_fraction | 0.000863 | . 9 week_fraction | 0.000058 | . 4 air_temperature_na | 0.000000 | . xs,y = to1.train.xs,to1.train.y valid_xs,valid_y = to1.valid.xs,to1.valid.y #y_pred,y_targs = learn.get_preds(ds_idx=0) #valid_y_pred,valid_y_targs = learn.get_preds() # Set negative values to 0 because the RMSLE metric can&#39;t handle negatives #y_pred[y_pred&lt;0]=0 #valid_y_pred[valid_y_pred&lt;0]=0 #print(&quot;Training set RMSLE: &quot; + str(np.sqrt(mean_squared_log_error(y_pred.detach().numpy(), y)))) #print(&quot;Validation set RMSLE: &quot; + str(np.sqrt(mean_squared_log_error(valid_y_pred.detach().numpy(), valid_y)))) print(&quot;Training set RMSLE: &quot; + str(mean_squared_log_error(y, m.predict(xs)))) print(&quot;Validation set RMSLE: &quot; + str(mean_squared_log_error(valid_y, m.predict(valid_xs)))) building=2 meter=2 #plt.scatter(valid_xs[(valid_xs[&#39;building_id&#39;]==building) &amp; (valid_xs[&#39;meter&#39;]==meter)][&#39;air_temperature&#39;], valid_y.loc[valid_xs[(valid_xs[&#39;building_id&#39;]==building) &amp; (valid_xs[&#39;meter&#39;]==meter)].index]) #plt.scatter(valid_xs[(valid_xs[&#39;building_id&#39;]==building) &amp; (valid_xs[&#39;meter&#39;]==meter)][&#39;air_temperature&#39;], pd.DataFrame(valid_y_pred, index=valid_y.index).loc[valid_xs[(valid_xs[&#39;building_id&#39;]==building) &amp; (valid_xs[&#39;meter&#39;]==meter)].index], color=&#39;orange&#39;) . . Training set RMSLE: 1.6625878178882256 Validation set RMSLE: 2.1919660764707207 . Learning Rates . We tried using lr_find() to choose a learning rate. However, there is so much hetrogeneity in the data that lr_find() doesn&#39;t give the lovely smooth curves in the documentation. Moreover, we found that the suggested learning rates were too high, resulting in divergence when training. . learn = tabular_learner(dls1, metrics=rmse) learn.summary() # Exploring learning rates learn.lr_find(stop_div=False, num_it=200) . SuggestedLRs(valley=0.14454397559165955) . Model Training . So, we trained a bunch of models, experimenting with all the hyperparameters discussed, including: . Features | Batch sizes | Learning rates | Number of buildings included | . learn.fit_one_cycle(5, lr_max=.0015) . epoch train_loss valid_loss _rmse time . 0 | 1160131.750000 | 54425892.000000 | 7377.400391 | 02:05 | . 1 | 1350446.625000 | 28693202.000000 | 5356.600098 | 02:06 | . 2 | 1303213.500000 | 5809114.000000 | 2410.210693 | 02:06 | . 3 | 1267145.375000 | 3458165.250000 | 1859.613647 | 02:07 | . 4 | 1447270.250000 | 3337492.000000 | 1826.883911 | 02:07 | . Metrics . The competition stated that that chosen metric was root mean squared log error: RMSLE=sqrt(1/ùëõ‚àë(log(ùëù+1)‚àílog(ùëé+1))**2) . fast.ai doesn&#39;t have this metric but does have mean squared log error, MSLE. The catch is that RMSLE and MSLE do not handle negative numbers, returning -‚àû. Our neural network predicts some negative numbers for meter readings that should be 0. So we chose to train our model using root mean squared error (RMSE) as our metric and then use RMSLE with negative values set to 0 when evaluating our performance manually post-training. . learn.show_results() from sklearn.metrics import mean_squared_log_error xs,y = to1.train.xs,to1.train.y valid_xs,valid_y = to1.valid.xs,to1.valid.y y_pred,y_targs = learn.get_preds(ds_idx=0) valid_y_pred,valid_y_targs = learn.get_preds() # Set negative values to 0 because the RMSLE metric can&#39;t handle negatives y_pred[y_pred&lt;0]=0 valid_y_pred[valid_y_pred&lt;0]=0 print(&quot;Training set RMSLE: &quot; + str(np.sqrt(mean_squared_log_error(y_pred.detach().numpy(), y)))) print(&quot;Validation set RMSLE: &quot; + str(np.sqrt(mean_squared_log_error(valid_y_pred.detach().numpy(), valid_y)))) building=2 meter=2 plt.scatter(valid_xs[(valid_xs[&#39;building_id&#39;]==building) &amp; (valid_xs[&#39;meter&#39;]==meter)][&#39;air_temperature&#39;], valid_y.loc[valid_xs[(valid_xs[&#39;building_id&#39;]==building) &amp; (valid_xs[&#39;meter&#39;]==meter)].index]) plt.scatter(valid_xs[(valid_xs[&#39;building_id&#39;]==building) &amp; (valid_xs[&#39;meter&#39;]==meter)][&#39;air_temperature&#39;], pd.DataFrame(valid_y_pred, index=valid_y.index).loc[valid_xs[(valid_xs[&#39;building_id&#39;]==building) &amp; (valid_xs[&#39;meter&#39;]==meter)].index], color=&#39;orange&#39;) . building_id meter site_id primary_use air_temperature_na dew_temperature_na air_temperature dew_temperature square_feet week_fraction day_fraction meter_reading meter_reading_pred . 0 215.0 | 1.0 | 3.0 | 6.0 | 1.0 | 1.0 | 1.461091 | 1.781272 | -0.019577 | 1.645413 | 1.080166 | 110.750000 | 99.642830 | . 1 169.0 | 1.0 | 2.0 | 7.0 | 1.0 | 1.0 | -0.490342 | -0.744237 | -0.891359 | -1.103503 | -0.800389 | 27.877399 | 38.520355 | . 2 24.0 | 3.0 | 1.0 | 7.0 | 1.0 | 1.0 | 2.079424 | 1.497241 | 1.639729 | 0.797637 | -1.379266 | 0.000000 | -361.954224 | . 3 146.0 | 4.0 | 2.0 | 10.0 | 1.0 | 1.0 | -0.693865 | -0.461389 | -0.846883 | -1.537670 | -0.366284 | 211.785995 | 321.119629 | . 4 218.0 | 3.0 | 3.0 | 7.0 | 1.0 | 1.0 | 0.286547 | 0.821089 | -0.029925 | -0.214609 | -1.523932 | 423.042999 | 438.246033 | . 5 192.0 | 4.0 | 2.0 | 2.0 | 1.0 | 1.0 | 0.952555 | 1.669237 | 0.610492 | 0.302535 | -1.379266 | 0.000000 | 1310.548584 | . 6 58.0 | 1.0 | 1.0 | 2.0 | 1.0 | 1.0 | 1.358517 | 2.002185 | 0.483638 | 1.231698 | 1.660103 | 378.428009 | 295.039764 | . 7 151.0 | 2.0 | 2.0 | 7.0 | 1.0 | 1.0 | 1.358517 | 1.609275 | -0.543888 | 1.521638 | 0.211957 | 177.897003 | 88.404243 | . 8 115.0 | 2.0 | 2.0 | 10.0 | 1.0 | 1.0 | 1.923396 | 2.286216 | -0.367320 | -0.669865 | -1.234494 | 1264.959961 | 1326.524536 | . Training set RMSLE: 1.5497105 Validation set RMSLE: 1.7401527 . &lt;matplotlib.collections.PathCollection at 0x7fa73dcd2150&gt; . We achieved a validation set RMSLE of 1.74, compared to validation set RMSLE of 2.19 for our DecisionTreeRegressor. . The winning Kaggle entry scored RMSLE of 1.23. Assuming that our cross-validated sets are representative of the test set (a big assumption!) this puts us around 3000th on the Kaggle leaderboard of 3500 entries. That&#39;s not great but not embarrasing either. . Next Steps . With more time, to improve our model, we would: . Consider different fill strategies for missing data, such as assuming that buildings without a &#39;year_built&#39; are old buildings rather than taking the median. | Separate building 1099 into a separate model, or normalise &#39;meter_reading&#39; per building/meter before inputting it into the model | Explore anomalies such as 0.0 (not missing) for a whole site for a particular day in winter | Check that the test dataset has similar statistical properties to the training/validation set | Ensembling with boosted and bagged decision trees | Submit to Kaggle for evaluation against the test set | .",
            "url": "https://fastai-energetic-engineering.github.io/ashrae/fastai/kaggle/2021/07/22/_07_23_tabular1online_presentation.html",
            "relUrl": "/fastai/kaggle/2021/07/22/_07_23_tabular1online_presentation.html",
            "date": " ‚Ä¢ Jul 22, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Getting Kaggle Data for ASHRAE Energy Prediction",
            "content": ". !pip install -Uqq fastbook import fastbook fastbook.setup_book() . . |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 727kB 8.1MB/s |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 194kB 35.5MB/s |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.2MB 36.4MB/s |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 51kB 9.3MB/s |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 61kB 10.1MB/s |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 61kB 10.9MB/s Mounted at /content/gdrive . from fastbook import * import os from google.colab import files import pandas as pd import datetime . . This notebook demonstrates how I downloaded the ASHRAE Energy Prediction Data from Kaggle. . First, we need to install the Kaggle API. . !pip install kaggle --upgrade -q . I will download the data into a folder in my google drive. First, I will set my home directory. . p = Path(&#39;drive/MyDrive/Colab Notebooks/ashrae/&#39;) os.chdir(p) # change directory . We need to download Kaggle API token and then put the .json file in .kaggle folder. We can upload the key directly from colab. . files.upload() # use this to upload your API json key !mkdir ~/.kaggle # create folder !cp kaggle.json ~/.kaggle/ # move the key into the folder !chmod 600 ~/.kaggle/kaggle.json # change permissions of the file . We can finally download the file! . os.chdir(&#39;data&#39;) # move to data folder !kaggle competitions download -c ashrae-energy-prediction . for item in os.listdir(): # for every item in the folder if item.endswith(&#39;.zip&#39;): # check if it is a .zip file file_extract(item) # if it is, then extract file os.remove(item) # and then remove the .zip . os.chdir(&quot;..&quot;) # return to initial folder . Joining Tables . Our training data comprised of three tables: . building_metadata.csv | weather_train.csv | train.csv | . We need to join the tables. First, let&#39;s see what&#39;s in the tables. . building = pd.read_csv(&#39;data/building_metadata.csv&#39;) weather = pd.read_csv(&#39;data/weather_train.csv&#39;) train = pd.read_csv(&#39;data/train.csv&#39;) . building contains the buildings&#39; metadata. . building.head() . site_id building_id primary_use square_feet year_built floor_count . 0 0 | 0 | Education | 7432 | 2008.0 | NaN | . 1 0 | 1 | Education | 2720 | 2004.0 | NaN | . 2 0 | 2 | Education | 5376 | 1991.0 | NaN | . 3 0 | 3 | Education | 23685 | 2002.0 | NaN | . 4 0 | 4 | Education | 116607 | 1975.0 | NaN | . site_id - Foreign key for the weather files. | building_id - Foreign key for training.csv | primary_use - Indicator of the primary category of activities for the building based on EnergyStar property type definitions | square_feet - Gross floor area of the building | year_built - Year building was opened | floor_count - Number of floors of the building | . weather contains weather data from the closest meteorological station. . weather.head() . site_id timestamp air_temperature cloud_coverage dew_temperature precip_depth_1_hr sea_level_pressure wind_direction wind_speed . 0 0 | 2016-01-01 00:00:00 | 25.0 | 6.0 | 20.0 | NaN | 1019.7 | 0.0 | 0.0 | . 1 0 | 2016-01-01 01:00:00 | 24.4 | NaN | 21.1 | -1.0 | 1020.2 | 70.0 | 1.5 | . 2 0 | 2016-01-01 02:00:00 | 22.8 | 2.0 | 21.1 | 0.0 | 1020.2 | 0.0 | 0.0 | . 3 0 | 2016-01-01 03:00:00 | 21.1 | 2.0 | 20.6 | 0.0 | 1020.1 | 0.0 | 0.0 | . 4 0 | 2016-01-01 04:00:00 | 20.0 | 2.0 | 20.0 | -1.0 | 1020.0 | 250.0 | 2.6 | . site_id | air_temperature - Degrees Celsius | cloud_coverage - Portion of the sky covered in clouds, in oktas | dew_temperature - Degrees Celsius | precip_depth_1_hr - Millimeters | sea_level_pressure - Millibar/hectopascals | wind_direction - Compass direction (0-360) | wind_speed - Meters per second | . Finally, train contains the target variable, meter reading, which represents energy consumption in kWh. . train.head() . building_id meter timestamp meter_reading . 0 0 | 0 | 2016-01-01 00:00:00 | 0.0 | . 1 1 | 0 | 2016-01-01 00:00:00 | 0.0 | . 2 2 | 0 | 2016-01-01 00:00:00 | 0.0 | . 3 3 | 0 | 2016-01-01 00:00:00 | 0.0 | . 4 4 | 0 | 2016-01-01 00:00:00 | 0.0 | . building_id - Foreign key for the building metadata. | meter - The meter id code. Read as {0: electricity, 1: chilledwater, 2: steam, 3: hotwater}. Not every building has all meter types. | timestamp - When the measurement was taken | meter_reading - The target variable. Energy consumption in kWh (or equivalent). | . Apparently there was some issues regarding the timestamps, as noted by this post. The timestamp in the weather and meter reading table were in GMT and local time, respectively. We have to keep this in mind before merging the tables. . Here I wrote a function that can prepare train and test data accordingly. . def prepare_data(type=&#39;train&#39;): assert type in [&#39;train&#39;, &#39;test&#39;] # read data building = pd.read_csv(&#39;data/building_metadata.csv&#39;) weather = pd.read_csv(f&#39;data/weather_{type}.csv&#39;) data = pd.read_csv(f&#39;data/{type}.csv&#39;) # convert datetime data[&#39;timestamp&#39;] = pd.to_datetime(data[&#39;timestamp&#39;]) # adjust timestamp timediff = {0:4,1:0,2:7,3:4,4:7,5:0,6:4,7:4,8:4,9:5,10:7,11:4,12:0,13:5,14:4,15:4} weather[&#39;time_diff&#39;]= weather[&#39;site_id&#39;].map(timediff) weather[&#39;time_diff&#39;] = weather[&#39;time_diff&#39;].apply(lambda x: datetime.timedelta(hours=x)) weather[&#39;timestamp_gmt&#39;] = pd.to_datetime(weather[&#39;timestamp&#39;]) weather[&#39;timestamp&#39;] = weather[&#39;timestamp_gmt&#39;] - weather[&#39;time_diff&#39;] # merge table data = data.merge(building, on=&#39;building_id&#39;, how=&#39;left&#39;) data = data.merge(weather, on=[&#39;site_id&#39;,&#39;timestamp&#39;], how=&#39;left&#39;) return data . Let&#39;s try this function out! . prepare_data(&#39;train&#39;).head() . building_id meter timestamp meter_reading site_id primary_use square_feet year_built floor_count air_temperature cloud_coverage dew_temperature precip_depth_1_hr sea_level_pressure wind_direction wind_speed time_diff timestamp_gmt . 0 0 | 0 | 2016-01-01 | 0.0 | 0 | Education | 7432 | 2008.0 | NaN | 20.0 | 2.0 | 20.0 | -1.0 | 1020.0 | 250.0 | 2.6 | 0 days 04:00:00 | 2016-01-01 04:00:00 | . 1 1 | 0 | 2016-01-01 | 0.0 | 0 | Education | 2720 | 2004.0 | NaN | 20.0 | 2.0 | 20.0 | -1.0 | 1020.0 | 250.0 | 2.6 | 0 days 04:00:00 | 2016-01-01 04:00:00 | . 2 2 | 0 | 2016-01-01 | 0.0 | 0 | Education | 5376 | 1991.0 | NaN | 20.0 | 2.0 | 20.0 | -1.0 | 1020.0 | 250.0 | 2.6 | 0 days 04:00:00 | 2016-01-01 04:00:00 | . 3 3 | 0 | 2016-01-01 | 0.0 | 0 | Education | 23685 | 2002.0 | NaN | 20.0 | 2.0 | 20.0 | -1.0 | 1020.0 | 250.0 | 2.6 | 0 days 04:00:00 | 2016-01-01 04:00:00 | . 4 4 | 0 | 2016-01-01 | 0.0 | 0 | Education | 116607 | 1975.0 | NaN | 20.0 | 2.0 | 20.0 | -1.0 | 1020.0 | 250.0 | 2.6 | 0 days 04:00:00 | 2016-01-01 04:00:00 | . That&#39;s it! In the next blogpost, I will show how to load this data into FastAI&#39;s dataloaders. .",
            "url": "https://fastai-energetic-engineering.github.io/ashrae/kaggle/preprocessing/2021/06/27/Getting-ASHRAE-Energy-Prediction-Data-from-Kaggle.html",
            "relUrl": "/kaggle/preprocessing/2021/06/27/Getting-ASHRAE-Energy-Prediction-Data-from-Kaggle.html",
            "date": " ‚Ä¢ Jun 27, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Us",
          "content": "Team Energetic Engineering was formed for the Queensland AI‚Äôs FastAI course group project. We focus on tabular data analysis using the ASHRAE - Great Energy Predictor dataset from Kaggle. We are a group of energetic learners comprising (in alphabetical order): . Mikhael Manurung | Oluwakayode Olamoyegun | Owen Lamont | Roger Butler | . As part of our individual learning journeys, we each will be contributing notebooks in this site. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats.¬†&#8617; . |",
          "url": "https://fastai-energetic-engineering.github.io/ashrae/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ ‚Äúsitemap.xml‚Äù | absolute_url }} | .",
          "url": "https://fastai-energetic-engineering.github.io/ashrae/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}