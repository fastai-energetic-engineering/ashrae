{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2021-07-23-tabular1online-presentation.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPMknNrfY4Lpfs+cSejslI0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fastai-energetic-engineering/ashrae/blob/master/_notebooks/2021-07-23-tabular1online-presentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHV6EsbPjFkG"
      },
      "source": [
        "# ASHRAE Energy Prediction\n",
        "> by Energetic Engineering, the tabular-1-online group in the 2021 fast.ai course\n",
        "\n",
        "- toc: true\n",
        "- branch: master\n",
        "- badges: true\n",
        "- comments: true\n",
        "- categories: [fastai, kaggle]\n",
        "- image: images/joey-kyber-Pihl8kTtX-s-unsplash.jpg\n",
        "- hide: false\n",
        "- search_exclude: false\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LI4-_-U4k7Yu"
      },
      "source": [
        "The [ASHRAE Great Energy Predictor III](https://www.kaggle.com/c/ashrae-energy-prediction) is a Kaggle competition to predict energy use in buildings.\n",
        "\n",
        "The training dataset is 3.2GB, containing 20 million energy meter readings for 1449 buildings over a year. Four energy meters are sampled: chilled water, electric, hot water, and steam. Each dependent measurement is given with independent variable metadata for the building and weather. The [Kaggle data page](https://www.kaggle.com/c/ashrae-energy-prediction/data) shows the available files, with histograms for each variable.\n",
        "\n",
        "Our goal was to create a model that predicts energy usage per building and per meter in that building, using fast.ai.\n",
        "\n",
        "We started by downloading the dataset from Kaggle, joining the tables, fixing timestamp inconsistencies and transforming some values into smaller datatypes to save memory. We aimed to work in Colab so memory was precious.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rtMY7GzkuK1"
      },
      "source": [
        "#collapse\n",
        "!pip install -Uqq fastbook dtreeviz\n",
        "import fastbook\n",
        "fastbook.setup_book()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIQ1Jl_ek1ni"
      },
      "source": [
        "#collapse\n",
        "import os\n",
        "import gc\n",
        "import pandas as pd\n",
        "import datetime as dt\n",
        "from tqdm.auto import tqdm\n",
        "from pandas.api.types import is_string_dtype, is_numeric_dtype, is_categorical_dtype\n",
        "from fastai.tabular.all import *\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from dtreeviz.trees import *\n",
        "from IPython.display import Image, display_svg, SVG\n",
        "\n",
        "pd.options.display.max_rows = 20\n",
        "pd.options.display.max_columns = 8"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Vg5r614vVL8"
      },
      "source": [
        "#collapse\n",
        "%cd '/content/gdrive/MyDrive/Colab Notebooks/ashrae'\n",
        "train_valid = pd.read_parquet(\"feature_enhanced_train_combined.parquet.snappy\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Np6Jdahvg7J"
      },
      "source": [
        "#collapse\n",
        "## Memory optimization\n",
        "\n",
        "# Original code from https://www.kaggle.com/gemartin/load-data-reduce-memory-usage by @gemartin\n",
        "# Modified to support timestamp type, categorical type\n",
        "# Modified to add option to use float16\n",
        "\n",
        "from pandas.api.types import is_datetime64_any_dtype as is_datetime\n",
        "from pandas.api.types import is_categorical_dtype\n",
        "\n",
        "def reduce_mem_usage(df, use_float16=False):\n",
        "    \"\"\"\n",
        "    Iterate through all the columns of a dataframe and modify the data type to reduce memory usage.        \n",
        "    \"\"\"\n",
        "    \n",
        "    start_mem = df.memory_usage().sum() / 1024**2\n",
        "    print(\"Memory usage of dataframe is {:.2f} MB\".format(start_mem))\n",
        "    \n",
        "    for col in df.columns:\n",
        "        if is_datetime(df[col]) or is_categorical_dtype(df[col]) or is_string_dtype(df[col]):\n",
        "            continue\n",
        "        col_type = df[col].dtype\n",
        "        \n",
        "        if col_type != object:\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            if str(col_type)[:3] == \"int\":\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)  \n",
        "            else:\n",
        "                if use_float16 and c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                    df[col] = df[col].astype(np.float16)\n",
        "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float64)\n",
        "        else:\n",
        "            df[col] = df[col].astype(\"category\")\n",
        "\n",
        "    end_mem = df.memory_usage().sum() / 1024**2\n",
        "    print(\"Memory usage after optimization is: {:.2f} MB\".format(end_mem))\n",
        "    print(\"Decreased by {:.1f}%\".format(100 * (start_mem - end_mem) / start_mem))\n",
        "    \n",
        "    return df\n",
        "\n",
        "train_valid = reduce_mem_usage(train_valid, use_float16=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDSCxzMgvlU1"
      },
      "source": [
        "We started by browsing exploratory data analysis notebooks posted to Kaggle. We learnt that this is a gnarly dataset, with many missing values and outliers. For example, the 'meter_reading' dependent variable has mostly small values but also some very large values:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWngehXZvVqh"
      },
      "source": [
        "train_valid[['meter_reading']].hist()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}